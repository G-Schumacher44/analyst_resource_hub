___
## ğŸ¯ Purpose

This guide covers the essential, reusable tasks for turning raw or semi-structured data into an analysis-ready form. It is the first step in building clean, reliable datasets for exploratory analysis, early-stage modeling, and pipeline integration.

---

## ğŸ“¦ 1. Schema + Structure Alignment

### ğŸ”¹ Confirm dataset structure

```python
df.shape  # (rows, columns)
df.columns
```

* Validate column count, presence, and order
* Compare against expected schema or dictionary

### ğŸ”¹ Data types check

```python
df.dtypes
df.info()
```

âœ”ï¸ Log non-numeric fields or object types that require conversion

---

## ğŸ”¢ 2. Data Type Enforcement

### Common fixes:

* `object â†’ string / category`
* `object â†’ datetime`
* Coerce numerics from symbols (`$`, `%`, etc.)

### Example:

```python
df['price'] = pd.to_numeric(df['price'].replace('[\$,]', '', regex=True))
df['date'] = pd.to_datetime(df['date'], errors='coerce')
```

âœ”ï¸ Explicitly cast booleans, integers, and categories

---

## ğŸ§¹ 3. Value Normalization

### Text Cleanup

* Strip whitespace, unify case, replace typos/symbols

```python
df['city'] = df['city'].str.strip().str.lower()
```

### Numeric Fixes

* Coerce currency, percentages, bad encodings
* Handle placeholder outliers (e.g. -999)

### Common Tools:

```python
.str.strip(), .str.replace(), pd.to_numeric(), .astype()
```

---

## ğŸ§¾ 4. Missing Data Handling

### ğŸ”¹ Diagnostics

```python
df.isnull().sum()
df.isnull().mean().sort_values(ascending=False)
```

### ğŸ”¹ Imputation Strategies

| Type        | Strategy                   |
| ----------- | -------------------------- |
| Numeric     | Mean / Median fill         |
| Categorical | Mode or "Missing" token    |
| Timestamp   | Use time reference or flag |

```python
df['age'] = df['age'].fillna(df['age'].median())
df['gender'] = df['gender'].fillna('Missing')
```

âœ”ï¸ Track imputed fields with binary flags if needed

---

## ğŸ“ 5. Outlier Detection (Light)

### ğŸ”¹ Z-score or IQR-based detection

```python
from scipy.stats import zscore
z = np.abs(zscore(df.select_dtypes(include='number')))
```

âœ”ï¸ Flag (not drop) unless critical

---

## ğŸ“Š 6. Categorical Grouping

### ğŸ”¹ Group rare categories

```python
freq = df['industry'].value_counts(normalize=True)
df['industry'] = df['industry'].where(freq > 0.01, 'Other')
```

âœ”ï¸ Use before encoding to reduce cardinality
âœ”ï¸ Review business logic to preserve interpretability

---

## ğŸ“‹ Analyst Checklist â€” Part 1

* [ ] Column names and schema matched
* [ ] Dtypes checked and enforced
* [ ] Currency / text normalized
* [ ] Missingness reviewed and imputed
* [ ] Z-score or IQR outliers flagged
* [ ] Categorical levels grouped if needed
* [ ] Dataset exported or copied to validated layer

---

## ğŸ’¡ Final Tip

> â€œPart 1 cleaning is your analysis launchpad â€” focus on clarity, consistency, and low-risk fixes.â€

Use this before: EDA, feature engineering, or modeling prep. Pair with: Validation Guide, Transformation Guide, and Cleaning Guide â€” Part 2.
