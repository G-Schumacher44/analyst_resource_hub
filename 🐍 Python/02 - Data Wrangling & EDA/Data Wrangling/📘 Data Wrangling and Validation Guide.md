___
## ğŸ¯ Purpose

This guide consolidates all critical tasks for cleaning, validating, and preparing structured and semi-structured data before analysis or modeling. It supports pipeline-ready workflows and schema-compliant data management across EDA, ML, and deployment.

---

## ğŸ“¦ 1. Initial Structural Review

### ğŸ”¹ Shape and Schema Alignment

```python
df.shape  # (rows, columns)
df.columns
df.info()
```

* Confirm row/column counts against expectations
* Check column presence, order, and spelling

### ğŸ”¹ Basic Summary Overview

```python
df.describe(include='all').T
```

* Detect constant fields, unexpected types, empty categories

âœ”ï¸ Save schema snapshot or comparison log for pipeline consistency

---

## ğŸ”¢ 2. Data Type and Conversion Review

| Type         | Action Needed                                  |
| ------------ | ---------------------------------------------- |
| `object`     | Detect true type (category, date, ID)          |
| `float64`    | Check for rounding issues, missingness         |
| `int64`      | Watch for encoding, special flags (e.g., -999) |
| `bool`       | Confirm logic vs stringified booleans          |
| `datetime64` | Parse formats, detect tz-aware timestamps      |

### Snippet:

```python
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df['flag'] = df['flag'].astype('bool')
```

âœ”ï¸ Log all casting and dtype overrides

---

## ğŸ§¹ 3. Value Normalization and Cleaning

### ğŸ”¹ Standard Cleaning Tasks

* Strip whitespace
* Lowercase strings / apply title case
* Remove symbols (`$`, `%`, `#`, etc.)
* Parse mixed-format numerics (e.g., `$1,200.00` â†’ 1200.0)

```python
df['name'] = df['name'].str.strip().str.lower()
df['cost'] = df['cost'].replace('[\$,]', '', regex=True).astype(float)
```

âœ”ï¸ Ensure consistent formatting across columns and rows

---

## ğŸ§ª 4. Missing Data Review

### ğŸ”¹ Diagnostics

```python
df.isnull().sum()
df.isnull().mean().sort_values(ascending=False)
```

### ğŸ”¹ Strategy Options

| Case                              | Suggested Strategy               |
| --------------------------------- | -------------------------------- |
| < 5% missing                      | Drop or fill with median/mode    |
| > 30% missing                     | Consider column drop             |
| Patterned missingness (MNAR, MAR) | Flag + model-based imputation    |
| Isolated categorical gaps         | Fill with "Missing" or new label |

âœ”ï¸ Always report imputation logic in data dictionary or logs

---

## ğŸ“ 5. Validation Rule Checks

| Rule Type          | Example                                           |
| ------------------ | ------------------------------------------------- |
| Allowed values     | `state âˆˆ ['CA', 'NY', 'TX']`                      |
| Value range bounds | `age âˆˆ [0, 120]`, `revenue >= 0`                  |
| Format constraints | Regex for emails, phone, postal codes             |
| Cross-field logic  | `start_date < end_date`, `qty > 0 if ordered = 1` |

### Snippet:

```python
df['valid_age'] = df['age'].between(0, 120)
df['flag_invalid_state'] = ~df['state'].isin(valid_states)
```

âœ”ï¸ Build reusable validation functions and triggers for logging

---

## ğŸ” 6. Categorical Review and Encoding Prep

### ğŸ”¹ Cardinality and Balance

```python
df['category'].value_counts(normalize=True)
df['category'].nunique()
```

âœ”ï¸ Group rare levels as "Other" when frequency < 1â€“5%
âœ”ï¸ Use ordinal vs one-hot vs target encoding as appropriate

---

## ğŸ“ 7. Outlier Handling and Flagging

### ğŸ”¹ Visual and Statistical Detection

* Boxplots, histograms, scatterplots
* Z-score, IQR range check, MAD (robust)

```python
from scipy.stats import zscore
outliers = (np.abs(zscore(df['feature'])) > 3)
```

âœ”ï¸ Create outlier flag column for traceability
âœ”ï¸ Avoid automatic deletion without business logic

---

## ğŸ§¾ 8. Schema and Audit Logging

| Task                        | Tool / Method                    |
| --------------------------- | -------------------------------- |
| Capture schema snapshot     | `df.dtypes.to_dict()`            |
| Save pre/post cleaning diff | Use `deepdiff`, `pandas-diff`    |
| Add processing metadata     | Log source, date, imputer, notes |

âœ”ï¸ Use structured metadata tracking per versioned dataset

---

## ğŸ“‹ Master Wrangling + Validation Checklist

* [ ] Column names + schema confirmed
* [ ] Dtypes explicitly validated and cast
* [ ] Missingness reviewed + handling strategy applied
* [ ] Strings/numbers parsed + normalized
* [ ] Outliers flagged and/or handled
* [ ] Categorical levels grouped and encoded
* [ ] Validation rules checked (range, format, logic)
* [ ] Schema snapshot or cleaning log saved

---

## ğŸ’¡ Final Tip

> â€œWrangling is not just cleanup â€” itâ€™s structure, strategy, and traceability. Build once, validate always.â€

Use before: Feature engineering, EDA, ML pipelines, or schema release.
